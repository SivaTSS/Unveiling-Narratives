{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a00572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f71b7f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94293109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_entity(entity):\n",
    "    \"\"\"\n",
    "    Preprocess the extracted person entity by:\n",
    "    - Stripping leading/trailing whitespace\n",
    "    - Removing surrounding quotes and other non-alphanumeric characters\n",
    "    - Replacing newline characters and excessive spaces\n",
    "    - Removing non-alphabetic characters except spaces and hyphens\n",
    "    - Standardizing capitalization to title case\n",
    "    \"\"\"\n",
    "    if not isinstance(entity, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    entity = entity.strip()\n",
    "    \n",
    "    # Remove surrounding quotes and other enclosing characters\n",
    "    entity = re.sub(r'^[\\\"\\'\\`]+|[\\\"\\'\\`]+$', '', entity)\n",
    "    \n",
    "    # Replace newline characters and excessive spaces with a single space\n",
    "    entity = re.sub(r'\\s+', ' ', entity)\n",
    "    \n",
    "    # Remove non-alphabetic characters except spaces and hyphens\n",
    "    entity = re.sub(r'[^\\w\\s-]', '', entity)\n",
    "    \n",
    "    # Standardize to title case\n",
    "    entity = entity.title()\n",
    "    \n",
    "    return entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85189838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_person(entity, min_length=2, max_length=100):\n",
    "    \"\"\"\n",
    "    Validates whether the preprocessed entity is a likely person name.\n",
    "\n",
    "    Parameters:\n",
    "    - entity (str): The preprocessed person entity.\n",
    "    - min_length (int): Minimum length of the name.\n",
    "    - max_length (int): Maximum length of the name.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if valid, False otherwise.\n",
    "    \"\"\"\n",
    "    if not entity:\n",
    "        return False\n",
    "    \n",
    "    # Length-based filtering\n",
    "    if len(entity) < min_length or len(entity) > max_length:\n",
    "        return False\n",
    "    \n",
    "    # Exclude entities with digits\n",
    "    if re.search(r'\\d', entity):\n",
    "        return False\n",
    "    \n",
    "    # Exclude entities that consist solely of uppercase letters (likely acronyms)\n",
    "    if entity.isupper() and len(entity) > 1:\n",
    "        return False\n",
    "    \n",
    "    # Exclude entities with two or more consecutive hyphens\n",
    "    if re.search(r'-{2,}', entity):\n",
    "        return False\n",
    "    \n",
    "    # Exclude entities containing 'And' or 'Or' as separate words\n",
    "    if re.search(r'\\b(?:And|Or)\\b', entity):\n",
    "        return False\n",
    "    \n",
    "    # Exclude entities prefixed with honorifics\n",
    "    honorifics = ['Miss', 'Mr', 'Mrs', 'Ms', 'Dr', 'Sir', 'Lady', 'Lord', 'Prof', 'Rev']\n",
    "    # Create a regex pattern to match any honorific at the start followed by a space\n",
    "    honorific_pattern = r'^(?:' + '|'.join(honorifics) + r')\\s+'\n",
    "    if re.match(honorific_pattern, entity):\n",
    "        return False\n",
    "    \n",
    "    # Additional heuristic: At least one uppercase character followed by lowercase (simple pattern)\n",
    "    if not re.search(r'[A-Z][a-z]+', entity):\n",
    "        return False\n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12a77d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def extract_valid_person_entities(text, nlp_model, chunk_size=5000):\n",
    "    \"\"\"\n",
    "    Extracts and validates unique person entities from the input text using spaCy by processing in chunks.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text to analyze.\n",
    "    - nlp_model: The loaded spaCy NLP model.\n",
    "    - chunk_size (int): Number of characters per chunk.\n",
    "\n",
    "    Returns:\n",
    "    - List of validated unique person entities.\n",
    "    \"\"\"\n",
    "    person_entities = []\n",
    "    doc = nlp_model(text)\n",
    "    \n",
    "    # Use spaCy's sentence segmentation to split the text into sentences\n",
    "    sentences = list(doc.sents)\n",
    "    \n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        sentence_text = sentence.text.strip()\n",
    "        # Check if adding this sentence exceeds the chunk size\n",
    "        if len(current_chunk) + len(sentence_text) + 1 > chunk_size:\n",
    "            # Process the current chunk\n",
    "            if current_chunk:\n",
    "                chunk_doc = nlp_model(current_chunk)\n",
    "                entities = [ent.text for ent in chunk_doc.ents if ent.label_ == 'PERSON']\n",
    "                person_entities.extend(entities)\n",
    "                current_chunk = \"\"\n",
    "        # Add the sentence to the current chunk\n",
    "        current_chunk += \" \" + sentence_text\n",
    "    \n",
    "    # Process any remaining text in the current chunk\n",
    "    if current_chunk:\n",
    "        chunk_doc = nlp_model(current_chunk)\n",
    "        entities = [ent.text for ent in chunk_doc.ents if ent.label_ == 'PERSON']\n",
    "        person_entities.extend(entities)\n",
    "    \n",
    "    # Preprocess entities\n",
    "    cleaned_entities = [preprocess_entity(entity) for entity in person_entities]\n",
    "    \n",
    "    # Validate entities\n",
    "    valid_entities = [entity for entity in cleaned_entities if is_valid_person(entity)]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    unique_valid_entities = list(set(valid_entities))\n",
    "    \n",
    "    return unique_valid_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95f9f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_entities_to_json(book_title, person_entities, output_dir):\n",
    "    \"\"\"\n",
    "    Saves the list of person entities for a book into a JSON file.\n",
    "    \n",
    "    Parameters:\n",
    "    - book_title (str): The title of the book.\n",
    "    - person_entities (list): List of validated person entities.\n",
    "    - output_dir (str): Directory where the JSON file will be saved.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Sanitize the book title to create a valid filename\n",
    "    sanitized_title = re.sub(r'[\\\\/*?:\"<>|]', \"\", book_title)\n",
    "    sanitized_title = sanitized_title.replace(' ', '_')  # Replace spaces with underscores for readability\n",
    "    \n",
    "    # Define the path for the individual JSON file\n",
    "    individual_json_path = os.path.join(output_dir, f\"{sanitized_title}_person_entities.json\")\n",
    "    \n",
    "    # Structure the data as a dictionary\n",
    "    book_data = {\n",
    "        'book_title': book_title,\n",
    "        'person_entities': person_entities\n",
    "    }\n",
    "    \n",
    "    # Save the person entities to the individual JSON file\n",
    "    try:\n",
    "        with open(individual_json_path, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(book_data, json_file, indent=4)\n",
    "        print(f\"Saved person entities for '{book_title}' to '{individual_json_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save '{book_title}' due to error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3fe786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_book(book_path, nlp_model, output_dir, chunk_size=1000000):\n",
    "    \"\"\"\n",
    "    Processes a single book: loads content, extracts and cleans person entities, and saves them.\n",
    "\n",
    "    Parameters:\n",
    "    - book_path (str): Path to the book's text file.\n",
    "    - nlp_model: The loaded spaCy NLP model.\n",
    "    - output_dir (str): Directory where the JSON file will be saved.\n",
    "    - chunk_size (int): Number of characters per chunk for processing.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    book_title = os.path.basename(book_path).replace('.txt', '').replace('_', ' ')\n",
    "    \n",
    "    try:\n",
    "        with open(book_path, 'r', encoding='utf-8') as file:\n",
    "            book_text = file.read()\n",
    "        \n",
    "        print(f\"Successfully loaded '{book_title}'.\")\n",
    "        \n",
    "        # Determine processing strategy based on text length\n",
    "        if len(book_text) <= nlp_model.max_length:\n",
    "            # Process the entire text\n",
    "            valid_persons = extract_valid_person_entities(book_text, nlp_model)\n",
    "        else:\n",
    "            # Process in chunks\n",
    "            print(f\"Text length ({len(book_text)}) exceeds max_length ({nlp_model.max_length}). Processing in chunks.\")\n",
    "            valid_persons = extract_valid_person_entities(text=book_text, nlp_model=nlp_model, chunk_size=chunk_size)\n",
    "        \n",
    "        print(f\"Number of valid person entities found in '{book_title}': {len(valid_persons)}\")\n",
    "        \n",
    "        # Save the valid entities to a JSON file\n",
    "        save_entities_to_json(book_title, valid_persons, output_dir)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing '{book_title}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f411dce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 'Agatha Christie   The Secret Adversary'.\n",
      "Number of valid person entities found in 'Agatha Christie   The Secret Adversary': 156\n",
      "Saved person entities for 'Agatha Christie   The Secret Adversary' to '../data/processed/ner_results/Agatha_Christie___The_Secret_Adversary_person_entities.json'.\n"
     ]
    }
   ],
   "source": [
    "CLEANED_DIR = '../data/selected_100_books/'\n",
    "OUTPUT_DIR = '../data/processed/ner_results/'\n",
    "single_book_path = os.path.join(CLEANED_DIR, 'Agatha Christie___The Secret Adversary.txt')\n",
    "process_single_book(single_book_path, spacy.load('en_core_web_sm'), OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "199e8ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_books(cleaned_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Processes all books in the specified directory to extract and save person entities.\n",
    "    \n",
    "    Parameters:\n",
    "    - cleaned_dir (str): Directory containing the cleaned book text files.\n",
    "    - output_dir (str): Directory where individual JSON files will be saved.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Get list of all cleaned text files in the selected 100 books directory\n",
    "    cleaned_files = glob.glob(os.path.join(cleaned_dir, '*.txt'))\n",
    "    \n",
    "    print(f\"Number of books to process: {len(cleaned_files)}\")\n",
    "    \n",
    "    # Initialize spaCy's English model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    # Iterate through each book and process\n",
    "    for book_path in tqdm(cleaned_files, desc=\"Processing Books\"):\n",
    "        process_single_book(book_path, nlp, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f31c032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of books to process: 101\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f6444f4fe449018761fed90ee92a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Books:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 'Agatha Christie   The Secret Adversary'.\n",
      "Number of valid person entities found in 'Agatha Christie   The Secret Adversary': 156\n",
      "Saved person entities for 'Agatha Christie   The Secret Adversary' to '../data/processed/ner_results/individual_books/Agatha_Christie___The_Secret_Adversary_person_entities.json'.\n",
      "Successfully loaded 'Alfred Russel Wallace   Island Life'.\n",
      "Text length (1057182) exceeds max_length (1000000). Processing in chunks.\n",
      "Error processing 'Alfred Russel Wallace   Island Life': [E088] Text of length 1057182 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\n",
      "Successfully loaded 'Andrew Lang   A Short History of Scotland'.\n",
      "Number of valid person entities found in 'Andrew Lang   A Short History of Scotland': 883\n",
      "Saved person entities for 'Andrew Lang   A Short History of Scotland' to '../data/processed/ner_results/individual_books/Andrew_Lang___A_Short_History_of_Scotland_person_entities.json'.\n",
      "Successfully loaded 'Andrew Lang   The Blue Fairy Book'.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(OUTPUT_DIR, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Process all books\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mprocess_all_books\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLEANED_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mprocess_all_books\u001b[1;34m(cleaned_dir, output_dir)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Iterate through each book and process\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m book_path \u001b[38;5;129;01min\u001b[39;00m tqdm(cleaned_files, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Books\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 21\u001b[0m     \u001b[43mprocess_single_book\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbook_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mprocess_single_book\u001b[1;34m(book_path, nlp_model, output_dir, chunk_size)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Determine processing strategy based on text length\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(book_text) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m nlp_model\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Process the entire text\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     valid_persons \u001b[38;5;241m=\u001b[39m \u001b[43mextract_valid_person_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbook_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Process in chunks\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText length (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(book_text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exceeds max_length (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnlp_model\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Processing in chunks.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mextract_valid_person_entities\u001b[1;34m(text, nlp_model, chunk_size)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mExtracts and validates unique person entities from the input text using spaCy by processing in chunks.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m- List of validated unique person entities.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m person_entities \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 16\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Use spaCy's sentence segmentation to split the text into sentences\u001b[39;00m\n\u001b[0;32m     19\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(doc\u001b[38;5;241m.\u001b[39msents)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1049\u001b[0m     doc \u001b[38;5;241m=\u001b[39m proc(doc, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomponent_cfg\u001b[38;5;241m.\u001b[39mget(name, {}))  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the directory containing the selected 100 books\n",
    "CLEANED_DIR = '../data/selected_100_books/'\n",
    "\n",
    "# Define the directory where individual JSON files will be saved\n",
    "OUTPUT_DIR = '../data/processed/ner_results/individual_books/'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Process all books\n",
    "process_all_books(cleaned_dir=CLEANED_DIR, output_dir=OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d66ecaed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32e761940fd4fddb9282232628db323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating book lengths:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The longest book is 'William Makepeace Thackeray   The Newcomes.txt' with 2036713 characters.\n",
      "\n",
      "Suggested nlp.max_length: 2136713\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing the selected 100 books\n",
    "CLEANED_DIR = '../data/selected_100_books/'\n",
    "\n",
    "def calculate_max_length(book_dir):\n",
    "    \"\"\"\n",
    "    Calculates the maximum character length among all books in the directory.\n",
    "\n",
    "    Parameters:\n",
    "    - book_dir (str): The directory containing the books.\n",
    "\n",
    "    Returns:\n",
    "    - max_length (int): The length of the longest book.\n",
    "    \"\"\"\n",
    "    # Get list of all text files in the directory\n",
    "    book_files = glob.glob(os.path.join(book_dir, '*.txt'))\n",
    "    \n",
    "    max_length = 0\n",
    "    longest_book = \"\"\n",
    "    \n",
    "    # Iterate through each book and calculate character count\n",
    "    for book_path in tqdm(book_files, desc=\"Calculating book lengths\"):\n",
    "        try:\n",
    "            with open(book_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                char_count = len(text)\n",
    "                # Update max_length if the current book is longer\n",
    "                if char_count > max_length:\n",
    "                    max_length = char_count\n",
    "                    longest_book = os.path.basename(book_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {book_path}: {e}\")\n",
    "    \n",
    "    print(f\"\\nThe longest book is '{longest_book}' with {max_length} characters.\")\n",
    "    return max_length\n",
    "\n",
    "# Calculate the maximum character length for all books\n",
    "max_length = calculate_max_length(CLEANED_DIR)\n",
    "\n",
    "# Print the suggested nlp.max_length based on the longest book\n",
    "print(f\"\\nSuggested nlp.max_length: {max_length + 100000}\")  # Adding some buffer space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6f5391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
